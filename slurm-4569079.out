device = cuda
Traceback (most recent call last):
  File "/project/p_gnn001/code/tsp/tsp/test.py", line 129, in <module>
    model.load_state_dict(checkpoint['model_state_dict'])
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for EdgePropertyPredictionModel:
	Missing key(s) in state_dict: "message_passing_layers.0.message_passing.module.attn_l", "message_passing_layers.0.message_passing.module.attn_r", "message_passing_layers.0.message_passing.module.bias", "message_passing_layers.0.message_passing.module.fc.weight", "message_passing_layers.0.feed_forward.1.module.0.weight", "message_passing_layers.0.feed_forward.1.module.0.bias", "message_passing_layers.0.feed_forward.1.module.2.weight", "message_passing_layers.0.feed_forward.1.module.2.bias", "message_passing_layers.1.message_passing.module.attn_l", "message_passing_layers.1.message_passing.module.attn_r", "message_passing_layers.1.message_passing.module.bias", "message_passing_layers.1.message_passing.module.fc.weight", "message_passing_layers.1.feed_forward.1.module.0.weight", "message_passing_layers.1.feed_forward.1.module.0.bias", "message_passing_layers.1.feed_forward.1.module.2.weight", "message_passing_layers.1.feed_forward.1.module.2.bias", "message_passing_layers.2.message_passing.module.attn_l", "message_passing_layers.2.message_passing.module.attn_r", "message_passing_layers.2.message_passing.module.bias", "message_passing_layers.2.message_passing.module.fc.weight", "message_passing_layers.2.feed_forward.1.module.0.weight", "message_passing_layers.2.feed_forward.1.module.0.bias", "message_passing_layers.2.feed_forward.1.module.2.weight", "message_passing_layers.2.feed_forward.1.module.2.bias", "message_passing_layers.3.message_passing.module.attn_l", "message_passing_layers.3.message_passing.module.attn_r", "message_passing_layers.3.message_passing.module.bias", "message_passing_layers.3.message_passing.module.fc.weight", "message_passing_layers.3.feed_forward.1.module.0.weight", "message_passing_layers.3.feed_forward.1.module.0.bias", "message_passing_layers.3.feed_forward.1.module.2.weight", "message_passing_layers.3.feed_forward.1.module.2.bias". 
	Unexpected key(s) in state_dict: "message_passing_layers.0.message_passing.attn_l", "message_passing_layers.0.message_passing.attn_r", "message_passing_layers.0.message_passing.bias", "message_passing_layers.0.message_passing.fc.weight", "message_passing_layers.0.feed_forward.1.0.weight", "message_passing_layers.0.feed_forward.1.0.bias", "message_passing_layers.0.feed_forward.1.2.weight", "message_passing_layers.0.feed_forward.1.2.bias", "message_passing_layers.1.message_passing.attn_l", "message_passing_layers.1.message_passing.attn_r", "message_passing_layers.1.message_passing.bias", "message_passing_layers.1.message_passing.fc.weight", "message_passing_layers.1.feed_forward.1.0.weight", "message_passing_layers.1.feed_forward.1.0.bias", "message_passing_layers.1.feed_forward.1.2.weight", "message_passing_layers.1.feed_forward.1.2.bias", "message_passing_layers.2.message_passing.attn_l", "message_passing_layers.2.message_passing.attn_r", "message_passing_layers.2.message_passing.bias", "message_passing_layers.2.message_passing.fc.weight", "message_passing_layers.2.feed_forward.1.0.weight", "message_passing_layers.2.feed_forward.1.0.bias", "message_passing_layers.2.feed_forward.1.2.weight", "message_passing_layers.2.feed_forward.1.2.bias", "message_passing_layers.3.message_passing.attn_l", "message_passing_layers.3.message_passing.attn_r", "message_passing_layers.3.message_passing.bias", "message_passing_layers.3.message_passing.fc.weight", "message_passing_layers.3.feed_forward.1.0.weight", "message_passing_layers.3.feed_forward.1.0.bias", "message_passing_layers.3.feed_forward.1.2.weight", "message_passing_layers.3.feed_forward.1.2.bias". 
	size mismatch for message_passing_layers.0.feed_forward.0.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.0.feed_forward.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.0.feed_forward.0.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.0.feed_forward.0.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.1.feed_forward.0.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.1.feed_forward.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.1.feed_forward.0.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.1.feed_forward.0.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.2.feed_forward.0.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.2.feed_forward.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.2.feed_forward.0.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.2.feed_forward.0.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.3.feed_forward.0.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.3.feed_forward.0.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.3.feed_forward.0.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
	size mismatch for message_passing_layers.3.feed_forward.0.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).
