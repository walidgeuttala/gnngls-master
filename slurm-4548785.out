device = cuda
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/project/p_gnn001/code/tsp/tsp/train2.py", line 333, in <module>
    main()
  File "/project/p_gnn001/code/tsp/tsp/train2.py", line 326, in main
    val_loss = run(args)
               ^^^^^^^^^
  File "/project/p_gnn001/code/tsp/tsp/train2.py", line 217, in run
    epoch_loss = train(model, train_loader, args.target, criterion, optimizer, device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/p_gnn001/code/tsp/tsp/train2.py", line 52, in train
    loss.backward()
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/p_gnngw/miniconda3/envs/cuda118/lib/python3.12/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [60480, 32, 4]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
